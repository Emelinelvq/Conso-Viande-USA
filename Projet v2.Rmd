---
title: "Projet_économétrie_Lantz"
author: "Thomas, Emeline, Benoît et Lucie"
date: "2026-01-12"
output: html_document
---
# Introduction 
Expliquer à faire : 
- pourquoi la consommation de viande est un sujet pertinent
- pourquoi les USA
- intuition économique de la demande

La consommation de viande constitue un enjeu économique majeur aux États-Unis, tant du point de vue des ménages que des politiques agricoles et environnementales (+ infos). Ce projet vise à estimer une fonction de demande agrégée de viande aux États-Unis à partir de données macroéconomiques annuelles, puis à réaliser des prévisions de consommation à l’horizon 2030.

## Consigne : 
Construire une fonction de demande de viande aux États-Unis, puis prévoir la consommation jusqu’en 2030, avec :
- tests économétriques complets
- intervalles de confiance pour les prévisions
--> construction d'une fonction de demande agrégée ? 

## Préparation de la session R 
```{r, include=TRUE}
# Ouverture des librairies 
# Spécificité du package readxl
remove.packages("readxl")
install.packages("readxl", type = "source")

# Installation des librairies
install.packages("lmtest")
install.packages("tseries")
install.packages("glmnet")

library(readxl)
library(lmtest)
library(tseries)
library(glmnet)
library(strucchange)
library(sandwich)
library(car)
library(tseries)
```

## Importer et préparer les données
Les données utilisées sont annuelles et couvrent la période 2004–2024. La consommation de viande est issue de l’USDA Economic Research Service (ERS), qui fournit des statistiques détaillées sur les produits animaux aux États-Unis.  
Les données de PIB et de population proviennent de la Banque mondiale (World Bank).  
L’indice des prix à la consommation ainsi que le prix moyen du bœuf sont extraits de la base FRED de la Federal Reserve Bank of St. Louis.
L’utilisation de sources institutionnelles garantit la fiabilité et la cohérence des séries statistiques employées dans l’analyse économétrique. 
La consommation de viande est mesurée en milliards de livres (retail equivalent of disappearance). Les variables explicatives incluent le PIB, la population, l’indice des prix à la consommation et le prix moyen du bœuf.
# load les données faut qu'on le fasse depuis le repo (git) si qqn peut regarder

# data <- read_excel("CV_USA.xlsx")
```{r}
data <- cv_usa
head(data)

# Convertir Year en variable numérique (au cas où)
data$Year <- as.numeric(data$Year)
```

## Transformation logarithmique et en réel 
```{r}
# Transformation logarithmique pour obtenir des élasticités + hypothèse de normalité + hypothèse de variance constante 
data_log <- data
data_log$CV <- log(data$CV)
data_log$PIB <- log(data$PIB)
data_log$Population <- log(data$Population)
data_log$CPI <- log(data$CPI)
data_log$Beef_price <- log(data$Beef_price)

plot(data_log$Year, data_log$CV, type="l", main="Log consommation de viande") # grosse chute de la consommation de viande en 2015. Pourquoi ? Comment est-ce qu'on l'explique ?

# Variable réel plutôt que nominal - pourquoi ? 
data_reel <- data
data_reel$CV <- log(data$CV/(data$CPI/100))
data_reel$PIB <- log(data$PIB/(data$Population*(data$CPI/100)))
data_reel$Beef_price <- log(data$Beef_price/(data$CPI/100))

#mettons tout ce joli monde dans une matrice
X_reel <- model.matrix(CV ~ PIB +  Beef_price, data = data_reel)[, -1]
CV_reel <- data_reel$CV
```
## Création du modèle de régression multiple
```{r}
# Création de trois modèles 
modele <- lm(CV ~ PIB + Population + CPI + Beef_price, data = data) # modèle régression linéaire avec données de base
modele_log <- lm(CV ~ PIB + Population + CPI + Beef_price, data = data_log) # modèle régression linéaire avec les données en log
modele_reel <- lm(CV ~ PIB + Beef_price, data = data_reel) # modèle régression linéaire avec les données en réel

# Résumé des modèles
#1 - modèle de base 
summary(modele)
#tous les paramètres sont pertinents sauf le prix un peu mois (Student)
#Le modèle en général est super p-value giga faible (Fisher)

#2 - modèle avec données log 
summary(modele_log)
#ça change des trucs et plusieurs paramètres perdent en importance

#3 - modèle avec données réelles 
summary(modele_reel)
CV_pred_reel <- predict(modele_reel, newx = X_reel)
r2 <- 1 - sum((CV_reel - CV_pred_reel)^2) / sum((CV_reel - mean(CV_reel))^2)
r2 # 0.6779808 - pas si mal 
```

## Problèmes et tests
```{r}
#1 - Test d'autocorrélation d'ordre 1 (Durbin-Watson)
dwtest(modele)
dwtest(modele_log)
dwtest(modele_reel)
# p-value < 0.05 et dans les trois 2 cas donc autocorrélation 
# Correction 
coeftest(modele_reel, vcov = NeweyWest(modele_reel, lag = 1)) # corrige les erreurs-types pour autocorrélation d’ordre 1 et hétéroscédasticité
# Ici on a bien le PIB et le prix du boeuf on un impact négatif sur notre prix de la viande mais ce n'est pas significatif. L'intercept est significatif - mais il représente quoi déjà ? 

# Autre correction ? 
data_reel$CV_lag1 <- c(NA, head(data_reel$CV, -1))
modele_ar1 <- lm(CV ~ PIB + Beef_price + CV_lag1, 
                 data = data_reel, 
                 na.action = na.exclude)
summary(modele_ar1) # là on a plus rien de significatif !?
bgtest(modele_ar1, order = 1) # plus de autocorrélation 

#2 - Test hétéroscédasticité (Breusch-Pagan)
bptest(modele)
bptest(modele_log)
bptest(modele_reel)
# Pas d'hétéroscédasticité 

# Visualisation 
plot(modele, which = 1)
plot(modele_log, which = 1)
plot(modele_reel, wich = 1) # On observe la tendance à la courbure des résidus ce qui n'est pas optimal

#3 - Test de la multicollinéarité (VIF (Variance Inflation Factor)) 
vif(modele)
vif(modele_log)
vif(modele_reel)
# Les valeurs supérieures à 10 indiquent un problème sérieux, ce qui justifie l’utilisation de variables par habitant et de prix réels.

#4 - Vérification de la normalité des résidus (Jarque-Bera)
jarque.bera.test(residuals(modele)) # Les résidus ne sont pas normaux mais de peux car p-value = 0.01367 < 0.05 

# Visualisation graphique 
qqnorm(residuals(modele))
qqline(residuals(modele)) # Ils semblent pourtant normaux graphiquement mais ne le sont pas selon le test 

# On change donc de modèle et on test sur le modèles log et en valeur réelle
jarque.bera.test(residuals(modele_log)) # Les résidus sont normaux
qqnorm(residuals(modele_log))
qqline(residuals(modele_log))

jarque.bera.test(residuals(modele_reel)) # Les résidus sont normaux
qqnorm(residuals(modele_reel))
qqline(residuals(modele_reel))

#5 - Test de la stabilité temporelle (Cusum)
# Cusum 
cusum <- efp(CV ~ PIB + Beef_price, data = data_reel, type = "Rec-CUSUM")
plot(cusum) # On observe une forte rupture de tendance 
# On fait donc un test de Chow pour 2011, date de la rupture
sctest(CV_reel ~ X_reel, type="Chow", point = 7) # p-value = 8.44e-06, il y a donc bien une rupture temporelle (mais aussi en 2006 et 2005, genre ttes les dates avant 2007 donc à voir quoi en dire et surtout à voir le Cusumsquare)

# CUMSUMSQ - test stabilité des coefficients du modèle dans le temps
y <- as.matrix(data_log$CV) # variable expliquée 
X <- cbind(
  1,                       # constante
  data_log$PIB,            # on ajoute les variables explicatives 
  data_log$Population,
  data_log$CPI,
  data_log$Beef_price
)
w <- recresid(y, X)       # résidus récursifs
CUSUMSQ <- cumsum(w^2) / sum(w^2) # construction du cusumsq
nrow(X) == length(y) # check de sécurité --> TRUE donc c'est bon pas de pb de longueur ou de NA 
X <- as.matrix(X)
y <- as.matrix(y)

# Visualisation graphique 
plot(CUSUMSQ,
     type = "l",
     lwd = 2,
     ylab = "CUSUM of Squares",
     xlab = "Temps",
     main = "Test CUSUMSQ – Stabilité du modèle")
abline(h = c(0.05, 0.95), lty = 2, col = "red")
# La courbe ne reste en pas entre les bornes - ça veut dire qu'il n'y a pas de stabilité du modèle 
# Il y a donc rupture structurelle donc au moins un coefficient change dans le temps


# Test adaptaion code cours 
n <- nrow(X)  # nombre d'observations
k <- ncol(X)-1
# Résidus récursifs
rr <- recresid(y, X)

# Carrés
rr2 <- rr^2

# Somme des carrés totale (pour normaliser)
scr <- sum(rr2)

# Statistique cumulé (r_s)
cumrr <- cumsum(rr2)/scr

# Indices
nk1 <- n - k - 1
kp2 <- k + 2
t1 <- 1:nk1
t2 <- (kp2):n

# Valeur critique c0 (exemple pour alpha=0.05)
c0 <- 0.18915  # à adapter si n ou k change

# Limites inférieure et supérieure
smin <- ((t2 - k) / (n - k)) - c0
smax <- ((t2 - k) / (n - k)) + c0

# On aligne les longueurs
cumrr_trunc <- cumrr[kp2:n]  # pour que cumrr, smin et smax aient même longueur

cusum2 <- cbind(smin, cumrr_trunc, smax)

# Graphique final
matplot(t2, cusum2, type = "l", lty = 1, col = c("red", "black", "red"),
        xlab = "Observations", ylab = "CUSUMSQ",
        main = "Test CUSUM of Squares")
legend("topleft", legend=c("smin","cumrr","smax"),
       col=c("red","black","red"), lty=1)
```

#RESUME :
#On a de l'autocorrélation mais pas d'hétéroscédasticité, les résidus sont normaux avec le log et il semble y avoir une rupture temporelle vers 2007 --> introduction d'une dummy. 
# Malheureusement on a de la multicolinéarité (non c'est bon normalement) - PAS de régression RIDGE 

```{r}
# Dummy variable post2011
# Création d’une variable dummy : 0 avant 2011, 1 à partir de 2011
data_reel$post2011 <- ifelse(data_reel$Year >= 2011, 1, 0)
modele_reel_dummy <- lm(CV ~ PIB + Beef_price + post2011, 
                      data = data_reel)
summary(modele_reel_dummy) # ça m'a l'air pas trop mal et relativement cohérent enfin à part pour le prix de la viande

# r2
summary(modele_reel_dummy)$r.squared       # R² classique
summary(modele_reel_dummy)$adj.r.squared   # R² ajusté (corrige le nombre de variables)

# test: interaction pour capturer l’effet spécifique sur certaines variables
modele_reel_dummy_inter <- lm(CV ~ PIB*post2011 + Beef_price*post2011, 
                            data = data_reel)
summary(modele_reel_dummy_inter) # je sais pas trop quoi en penser c'est bizarre comme modèle - pas significatif 
```







#Déterminons maintenant un intervalle de confince pour notre joli modèle, par une méthode de bootstrap percentile-t avec un intervalle de confiance basé sur une statistique pivot (je cite le prof)

# Bootstrap
B <- 1000
pred_boot <- matrix(NA, nrow = B, ncol = nrow(X_reel))  # on stocke la prédiction pour chaque obs

set.seed(123)
for(b in 1:B){
  # Tirage bootstrap
  idx <- sample(1:nrow(X_log), replace = TRUE) # il vient d'où ce X_log 
  Xb <- X_reel[idx, ]
  yb <- CV_reel[idx]
  
  # Ajustement Ridge
  cvb <- cv.glmnet(Xb, yb, alpha = 0, nfolds = 5)
  
  # Prédiction pour les mêmes observations (jeu original)
  pred_boot[b, ] <- predict(cvb, newx = X_reel, s = "lambda.1se")
}

# Calcul IC 95% (percentile)
ci_lower <- apply(pred_boot, 2, quantile, probs = 0.025)
ci_upper <- apply(pred_boot, 2, quantile, probs = 0.975)

# Résultat final
result <- data.frame(
  CV_orig = CV_reel,
  pred_mean = apply(pred_boot, 2, mean),
  ci_lower = ci_lower,
  ci_upper = ci_upper
)
result

#la moyenne pour avoir un intervalle de confiance pour notre modèle
#y'a bcp de Chat GPT dans cette partie, il faut être vigilant mais je galère avec le R pr tout relire et être sur de moi
pred_mean_global <- mean(result$pred_mean)
ci_lower_global <- mean(result$ci_lower)
ci_upper_global <- mean(result$ci_upper)

pred_mean_global
ci_lower_global
ci_upper_global


# Bootstrap avec modèle corrigé 
B <- 1000  # nombre de répétitions
n <- nrow(data_reel)
pred_boot <- matrix(NA, nrow = B, ncol = n)  # pour stocker les prédictions de chaque bootstrap

set.seed(123)

for(b in 1:B){
  # Tirage bootstrap avec remise
  idx <- sample(1:n, replace = TRUE)
  data_b <- data_reel[idx, ]
  
  # Ajustement du modèle sur l'échantillon bootstrap
  model_b <- lm(CV ~ PIB + Beef_price + post2011, data = data_b)
  
  # Prédiction sur le jeu original
  pred_boot[b, ] <- predict(model_b, newdata = data_reel)
}

# calcul des intervalles de confiance 
# IC 95% pour chaque observation (percentile)
ci_lower <- apply(pred_boot, 2, quantile, probs = 0.025)
ci_upper <- apply(pred_boot, 2, quantile, probs = 0.975)

# Moyenne des prédictions
pred_mean <- apply(pred_boot, 2, mean)

# 
result <- data.frame(
  CV_orig = data_reel$CV,
  pred_mean = pred_mean,
  ci_lower = ci_lower,
  ci_upper = ci_upper
)

head(result)

colSums(is.na(data_reel))

#Partie 4
#Réussir à trouver comment obtenir des prévisions jusqu'à 2030 pour pouvoir faire nos prévision avec ce super modèle au R² dégueu !
# Lucie - je pense que peut-être on peut regarder au moins des prévisions de l'état américain 
#sur qqs variables qui vont nous permettre de dire au modèle dans quel direction il faut aller
 







